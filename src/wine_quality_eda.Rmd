---
title: "Exploratory data analysis of the Wine quality data set"
output: github_document
#bibliography: ../doc/wine_quality_refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
library(tidyverse)
library(knitr)
library(caret)
library(janitor)
library(reshape2)
library(viridis)
set.seed(2020)
```

## Summary of the data set

This dataset is about red variants of the Portuguese "Vinho Verde" wine. For more details, consult the reference [Cortez et al., 2009]. It was provided on [Kaggle](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009), but it was originally sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).

Each row in the data set represents several metrics of a certain red wine gained from its physicochemical tests such as acidity, sugar, chlorides, sulfur dioxide and alcohol etc.  The last column for each row represents the quality score. A wine's quality score is measured by sensors, which is score from 0 to 10, with 0 being the lowest and 10 being the highest. We will build a model to discover the relationship between the physicochemical measurements and quality score.

``` {r load data}
df <- read_csv('../data/winequality-red.csv')
df <- clean_names(df)
```

## Partition the data set into training and test sets
We will split the data into the training set with 75% of observations and test set with 25% of observations. Below are the counts of observations for each class:

```{r partition}
# split into training and test data sets
training_rows <- df %>% 
    mutate(quality = as.factor(quality)) %>% 
    select(quality) %>% 
    pull() %>% 
    createDataPartition(p = 0.8, list = FALSE)

training_data <- df %>% slice(training_rows)
test_data <- df %>% slice(-training_rows)

train_counts <- summarise(training_data, `Data partition` = "Training set",
                          `0` = sum(quality  == "0"),
                          `1` = sum(quality  == "1"), 
                          `2` = sum(quality  == "2"),
                          `3` = sum(quality  == "3"), 
                          `4` = sum(quality  == "4"),
                          `5` = sum(quality  == "5"), 
                          `6` = sum(quality  == "6"),
                          `7` = sum(quality  == "7"), 
                          `8` = sum(quality  == "8"),
                          `9` = sum(quality  == "9"))
kable(train_counts,
      caption = "Table 1. Counts of observation for each quality.")
```

```{r quality value distribution, fig.width=8, fig.height=5}
training_data %>% 
  ggplot(aes(quality)) + 
  geom_histogram(bins = 15)
```
Figure 1. Distribution of training set targets(y).

The quality classes of red wines are very **imbalanced**, this is definitely something we want to pay attention to. Most wines are among the middle classes(5 to 7), very few wines than excellent(class 8) or poor ones(class 3,4).

## Exploratory analysis on the training data set

### Distributions of all features

``` {r distributions of all features, fig.width=20, fig.height=15}
training_data %>% 
  gather(key = feature, value = value, -quality) %>% 
  mutate(feature = str_replace_all(feature, "_", " ")) %>% 
  ggplot(aes(x = value)) +
      facet_wrap(. ~ feature, scale = "free", ncol = 3) +
      geom_histogram(bins = 50)
```
Figure 2. Distribution of all features in the training set.

The histograms of all the features do not show any anomaly here.

### Correlation between features
``` {r heatmap, fig.width=20, fig.height=15}
corr <- cor(df)
melted_corr <- melt(corr)
# Heatmap

ggplot(data = melted_corr, aes(x=Var1, y=Var2, fill=value)) + geom_tile() +
theme(axis.text = element_text(size = 12),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```
Figure 3. Correlation heatmap between features.

From the heatmap, I can see the following combinations of features are positive related:
    
1. `fixed_acidity` and `citric_acid`
2. `fixed_acidity` and `density`
3. `total_sulfur_dioxide` and `free_sulfur_dioxide`

For some chemical reasons, these three sets of features are highly correlated, and we definitely need to be aware of this when building our machine learning model.
